import { llmOpenAI as llmOpenAIProvider, llmOpenAIResponses as llmOpenAIResponsesProvider } from "./llms/openai.js";
export { llmAnthropic } from "./llms/anthropic.js";
export { llmLlama } from "./llms/llama.js";
export { llmMistral } from "./llms/mistral.js";
export { llmBedrock } from "./llms/bedrock.js";
export { llmVertexStudio } from "./llms/vertex-studio.js";
export { llmAzure } from "./llms/azure.js";
export { createVolcanoTelemetry, noopTelemetry } from "./telemetry.js";
export type { VolcanoTelemetryConfig, VolcanoTelemetry } from "./telemetry.js";
export type { OpenAIConfig, OpenAIOptions } from "./llms/openai.js";
export type { AnthropicConfig, AnthropicOptions } from "./llms/anthropic.js";
export type { LlamaConfig, LlamaOptions } from "./llms/llama.js";
export type { MistralConfig, MistralOptions } from "./llms/mistral.js";
export type { BedrockConfig, BedrockOptions } from "./llms/bedrock.js";
export type { VertexStudioConfig, VertexStudioOptions } from "./llms/vertex-studio.js";
export type { AzureConfig, AzureOptions } from "./llms/azure.js";
import type { LLMHandle, ToolDefinition, LLMToolResult } from "./llms/types.js";
export type { LLMHandle, ToolDefinition, LLMToolResult };
export declare const llmOpenAI: typeof llmOpenAIProvider;
export declare const llmOpenAIResponses: typeof llmOpenAIResponsesProvider;
export interface VolcanoErrorMeta {
    stepId?: number;
    provider?: string;
    requestId?: string;
    retryable?: boolean;
}
export declare class VolcanoError extends Error {
    meta: VolcanoErrorMeta;
    constructor(message: string, meta?: VolcanoErrorMeta, options?: {
        cause?: unknown;
    });
}
export declare class AgentConcurrencyError extends VolcanoError {
}
export declare class TimeoutError extends VolcanoError {
}
export declare class ValidationError extends VolcanoError {
}
export declare class RetryExhaustedError extends VolcanoError {
}
export declare class LLMError extends VolcanoError {
}
export declare class MCPError extends VolcanoError {
}
export declare class MCPConnectionError extends MCPError {
}
export declare class MCPToolError extends MCPError {
}
export type MCPAuthConfig = {
    type: 'oauth' | 'bearer';
    token?: string;
    clientId?: string;
    clientSecret?: string;
    tokenEndpoint?: string;
    scope?: string;
};
export type MCPHandle = {
    listTools: () => Promise<{
        tools: Array<{
            name: string;
            description?: string;
            inputSchema?: any;
        }>;
    }>;
    callTool: (name: string, args: Record<string, any>) => Promise<any>;
    id: string;
    url: string;
    auth?: MCPAuthConfig;
};
/**
 * Connect to an MCP (Model Context Protocol) server via HTTP.
 * Supports connection pooling, OAuth/Bearer authentication, and automatic reconnection.
 *
 * @param url - HTTP endpoint URL for the MCP server (e.g., "http://localhost:3000/mcp")
 * @param options - Optional authentication configuration (OAuth 2.1 or Bearer token)
 * @returns MCPHandle for listing and calling tools
 *
 * @example
 * // Basic usage
 * const weather = mcp("http://localhost:3000/mcp");
 * const tools = await weather.listTools();
 * const forecast = await weather.callTool("get_forecast", { city: "San Francisco" });
 *
 * @example
 * // With OAuth authentication
 * const github = mcp("https://api.github.com/mcp", {
 *   auth: {
 *     type: 'oauth',
 *     clientId: process.env.GITHUB_CLIENT_ID!,
 *     clientSecret: process.env.GITHUB_SECRET!,
 *     tokenUrl: 'https://github.com/login/oauth/access_token'
 *   }
 * });
 */
export declare function mcp(url: string, options?: {
    auth?: MCPAuthConfig;
}): MCPHandle;
export declare function __internal_validateToolArgs(schema: any, args: any): void;
export declare function __internal_getMcpPoolStats(): {
    size: number;
    entries: {
        url: string;
        busyCount: number;
        lastUsed: number;
    }[];
};
export declare function __internal_forcePoolCleanup(): Promise<void>;
export declare function __internal_setPoolConfig(max: number, idleMs: number): void;
export declare function __internal_clearOAuthTokenCache(): void;
export declare function __internal_getOAuthTokenCache(): {
    endpoint: string;
    token: string;
    expiresAt: number;
}[];
/**
 * Discover all available tools from one or more MCP servers.
 * Results are cached for 60 seconds to improve performance.
 *
 * @param handles - Array of MCP handles to query for tools
 * @returns Combined array of all available tools from all servers
 *
 * @example
 * const weather = mcp("http://localhost:3000/mcp");
 * const calendar = mcp("http://localhost:4000/mcp");
 * const tools = await discoverTools([weather, calendar]);
 * console.log(tools.map(t => t.name)); // ["get_forecast", "create_event", ...]
 */
export declare function discoverTools(handles: MCPHandle[]): Promise<ToolDefinition[]>;
export declare function __internal_clearDiscoveryCache(): void;
export declare function __internal_setDiscoveryTtl(ms: number): void;
export declare function __internal_primeDiscoveryCache(handle: MCPHandle, rawTools: Array<{
    name: string;
    inputSchema?: any;
    description?: string;
}>): void;
export type RetryConfig = {
    delay?: number;
    backoff?: number;
    retries?: number;
};
/**
 * Metadata provided to stream-level onToken callback.
 * Allows conditional processing based on whether step-level handler already processed the token.
 *
 * @property stepIndex - Index of the current step (0-based)
 * @property handledByStep - True if step-level onToken handled this token. Use this to avoid double-processing.
 * @property stepPrompt - The prompt for this step (useful for conditional formatting)
 * @property llmProvider - The LLM provider ID (e.g., "OpenAI-gpt-4o-mini", "Anthropic-claude-3")
 *
 * @example
 * .stream({
 *   onToken: (token, meta) => {
 *     if (!meta.handledByStep) {
 *       // Only process if step didn't handle it
 *       res.write(`data: ${token}\n\n`);
 *     }
 *     // Always log analytics
 *     analytics.track(token, meta.stepIndex);
 *   }
 * })
 */
export type TokenMetadata = {
    stepIndex: number;
    handledByStep: boolean;
    stepPrompt?: string;
    llmProvider?: string;
};
/**
 * Options for the stream() method.
 * Supports both token-level and step-level callbacks for maximum flexibility.
 *
 * @property onToken - Called for each token as it arrives (with metadata).
 *                     Step-level onToken takes precedence - when a step has its own onToken,
 *                     this callback won't receive tokens from that step (meta.handledByStep will be true).
 * @property onStep - Called when each step completes. Equivalent to the callback in stream(callback).
 *
 * @example
 * // Both token and step callbacks
 * .stream({
 *   onToken: (token, meta) => {
 *     console.log(`Token from step ${meta.stepIndex}: ${token}`);
 *   },
 *   onStep: (step, index) => {
 *     console.log(`Step ${index} complete: ${step.durationMs}ms`);
 *   }
 * })
 *
 * @example
 * // Backward compatible: just a callback
 * .stream((step, index) => {
 *   console.log(`Step ${index} done`);
 * })
 */
export type StreamOptions = {
    onToken?: (token: string, meta: TokenMetadata) => void;
    onStep?: (step: StepResult, stepIndex: number) => void;
};
export type Step = {
    prompt: string;
    llm?: LLMHandle;
    instructions?: string;
    timeout?: number;
    retry?: RetryConfig;
    contextMaxChars?: number;
    contextMaxToolResults?: number;
    pre?: () => void;
    post?: () => void;
    onToken?: (token: string) => void;
} | {
    mcp: MCPHandle;
    tool: string;
    args?: Record<string, any>;
    timeout?: number;
    retry?: RetryConfig;
    contextMaxChars?: number;
    contextMaxToolResults?: number;
    pre?: () => void;
    post?: () => void;
} | {
    prompt: string;
    llm?: LLMHandle;
    mcp: MCPHandle;
    tool: string;
    args?: Record<string, any>;
    instructions?: string;
    timeout?: number;
    retry?: RetryConfig;
    contextMaxChars?: number;
    contextMaxToolResults?: number;
    pre?: () => void;
    post?: () => void;
} | {
    prompt: string;
    llm?: LLMHandle;
    mcps: MCPHandle[];
    instructions?: string;
    timeout?: number;
    retry?: RetryConfig;
    contextMaxChars?: number;
    contextMaxToolResults?: number;
    maxToolIterations?: number;
    pre?: () => void;
    post?: () => void;
    onToken?: (token: string) => void;
} | {
    prompt: string;
    llm?: LLMHandle;
    agents: AgentBuilder[];
    instructions?: string;
    timeout?: number;
    retry?: RetryConfig;
    contextMaxChars?: number;
    contextMaxToolResults?: number;
    maxAgentIterations?: number;
    pre?: () => void;
    post?: () => void;
};
export type StepResult = {
    prompt?: string;
    llmOutput?: string;
    durationMs?: number;
    llmMs?: number;
    mcp?: {
        endpoint: string;
        tool: string;
        result: any;
        ms?: number;
    };
    toolCalls?: Array<{
        name: string;
        arguments?: Record<string, any>;
        endpoint: string;
        result: any;
    } & {
        ms?: number;
    }>;
    parallel?: Record<string, StepResult>;
    parallelResults?: StepResult[];
    totalDurationMs?: number;
    totalLlmMs?: number;
    totalMcpMs?: number;
};
type StepFactory = (history: StepResult[]) => Step;
export interface AgentBuilder {
    name?: string;
    description?: string;
    resetHistory(): AgentBuilder;
    then(s: Step | StepFactory): AgentBuilder;
    parallel(stepsOrDict: Step[] | Record<string, Step>, hooks?: {
        pre?: () => void;
        post?: () => void;
    }): AgentBuilder;
    branch(condition: (history: StepResult[]) => boolean, branches: {
        true: (agent: AgentBuilder) => AgentBuilder;
        false: (agent: AgentBuilder) => AgentBuilder;
    }, hooks?: {
        pre?: () => void;
        post?: () => void;
    }): AgentBuilder;
    switch<T = string>(selector: (history: StepResult[]) => T, cases: Record<string, (agent: AgentBuilder) => AgentBuilder> & {
        default?: (agent: AgentBuilder) => AgentBuilder;
    }, hooks?: {
        pre?: () => void;
        post?: () => void;
    }): AgentBuilder;
    while(condition: (history: StepResult[]) => boolean, body: (agent: AgentBuilder) => AgentBuilder, opts?: {
        maxIterations?: number;
        timeout?: number;
        pre?: () => void;
        post?: () => void;
    }): AgentBuilder;
    forEach<T>(items: T[], body: (item: T, agent: AgentBuilder) => AgentBuilder, hooks?: {
        pre?: () => void;
        post?: () => void;
    }): AgentBuilder;
    retryUntil(body: (agent: AgentBuilder) => AgentBuilder, successCondition: (result: StepResult) => boolean, opts?: {
        maxAttempts?: number;
        backoff?: number;
        pre?: () => void;
        post?: () => void;
    }): AgentBuilder;
    runAgent(subAgent: AgentBuilder, hooks?: {
        pre?: () => void;
        post?: () => void;
    }): AgentBuilder;
    run(log?: (s: StepResult, stepIndex: number) => void): Promise<StepResult[]>;
    stream(optionsOrLog?: StreamOptions | ((s: StepResult, stepIndex: number) => void)): AsyncGenerator<StepResult, void, unknown>;
}
type AgentOptions = {
    llm?: LLMHandle;
    instructions?: string;
    name?: string;
    description?: string;
    hideProgress?: boolean;
    timeout?: number;
    retry?: RetryConfig;
    contextMaxChars?: number;
    contextMaxToolResults?: number;
    mcpAuth?: Record<string, MCPAuthConfig>;
    telemetry?: import('./telemetry.js').VolcanoTelemetry;
    maxToolIterations?: number;
};
/**
 * Create an AI agent that chains LLM reasoning with MCP tool calls.
 *
 * @param opts - Optional configuration including LLM provider, instructions, timeout, retry policy, and observability
 * @returns AgentBuilder for chaining steps with .then(), run(), and stream()
 *
 * @example
 * // Simple agent
 * const results = await agent({ llm: llmOpenAI({...}) })
 *   .then({ prompt: "Analyze data" })
 *   .then({ prompt: "Generate insights" })
 *   .run();
 *
 * @example
 * // With automatic tool selection
 * await agent({ llm })
 *   .then({
 *     prompt: "Book a meeting and send confirmation",
 *     mcps: [calendar, email]
 *   })
 *   .run();
 */
export declare function agent(opts?: AgentOptions): AgentBuilder;
