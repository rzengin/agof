import OpenAI from "openai";
import { createOpenAICompatibleTools, parseOpenAICompatibleResponse } from "./utils.js";
export function llmOpenAI(cfg) {
    if (!cfg.model) {
        throw new Error("llmOpenAI: Missing required 'model' parameter. " +
            "Please specify which OpenAI model to use. " +
            "Example: llmOpenAI({ apiKey: 'sk-...', model: 'gpt-5-mini' })");
    }
    const model = cfg.model;
    const id = `OpenAI-${model}`;
    const client = new OpenAI({ apiKey: cfg.apiKey, baseURL: cfg.baseURL });
    const options = cfg.options || {};
    return {
        id,
        client,
        model,
        gen: async (prompt) => {
            const r = await client.chat.completions.create({
                model,
                messages: [{ role: "user", content: prompt }],
                ...options,
            });
            return r.choices?.[0]?.message?.content ?? "";
        },
        genWithTools: async (prompt, tools) => {
            const { nameMap, formattedTools } = createOpenAICompatibleTools(tools);
            const r = await client.chat.completions.create({
                model,
                messages: [{ role: "user", content: prompt }],
                tools: formattedTools,
                tool_choice: "auto",
                ...options,
            });
            const message = r.choices?.[0]?.message;
            return parseOpenAICompatibleResponse(message, nameMap);
        },
        genStream: async function* (prompt) {
            const stream = await client.chat.completions.create({
                model,
                messages: [{ role: "user", content: prompt }],
                stream: true,
                ...options,
            });
            for await (const chunk of stream) {
                const delta = chunk?.choices?.[0]?.delta?.content;
                if (typeof delta === "string" && delta.length > 0) {
                    yield delta;
                }
            }
        }
    };
}
/**
 * OpenAI LLM with Structured Outputs (JSON Schema validation)
 * Uses Chat Completions API with response_format set to json_schema mode.
 * Guarantees the model's output matches your JSON schema.
 *
 * Supported models: gpt-4o-mini, gpt-4o, gpt-4o-2024-08-06 and later, o1, o3
 */
export function llmOpenAIResponses(cfg) {
    if (!cfg.model) {
        throw new Error("llmOpenAIResponses: Missing required 'model' parameter. " +
            "Please specify which OpenAI model to use. " +
            "Supported: gpt-4o-mini, gpt-4o, o1-mini, o1-preview, o3-mini. " +
            "Example: llmOpenAIResponses({ apiKey: 'sk-...', model: 'gpt-4o-mini', options: { jsonSchema: {...} } })");
    }
    if (!cfg.options?.jsonSchema) {
        throw new Error("llmOpenAIResponses: Missing required 'jsonSchema' in options. " +
            "Structured outputs require a JSON schema. " +
            "Example: options: { jsonSchema: { name: 'response', schema: { type: 'object', properties: {...} } } }");
    }
    const model = cfg.model;
    const id = `OpenAI-Responses-${model}`;
    const client = new OpenAI({ apiKey: cfg.apiKey, baseURL: cfg.baseURL });
    const { jsonSchema, ...otherOptions } = cfg.options;
    // Build response_format with json_schema mode
    const responseFormat = {
        type: "json_schema",
        json_schema: {
            name: jsonSchema.name,
            description: jsonSchema.description,
            schema: jsonSchema.schema,
            strict: jsonSchema.strict ?? true // Default to strict mode
        }
    };
    return {
        id,
        client,
        model,
        gen: async (prompt) => {
            const r = await client.chat.completions.create({
                model,
                messages: [{ role: "user", content: prompt }],
                response_format: responseFormat,
                ...otherOptions,
            });
            return r.choices?.[0]?.message?.content ?? "";
        },
        genWithTools: async (prompt, tools) => {
            const { nameMap, formattedTools } = createOpenAICompatibleTools(tools);
            const r = await client.chat.completions.create({
                model,
                messages: [{ role: "user", content: prompt }],
                tools: formattedTools,
                tool_choice: "auto",
                response_format: responseFormat,
                ...otherOptions,
            });
            const message = r.choices?.[0]?.message;
            return parseOpenAICompatibleResponse(message, nameMap);
        },
        genStream: async function* (prompt) {
            const stream = await client.chat.completions.create({
                model,
                messages: [{ role: "user", content: prompt }],
                stream: true,
                response_format: responseFormat,
                ...otherOptions,
            });
            for await (const chunk of stream) {
                const delta = chunk?.choices?.[0]?.delta?.content;
                if (typeof delta === "string" && delta.length > 0) {
                    yield delta;
                }
            }
        }
    };
}
