import { createOpenAICompatibleTools, parseOpenAICompatibleResponse } from "./utils.js";
export function llmLlama(cfg) {
    if (!cfg.model) {
        throw new Error("llmLlama: Missing required 'model' parameter. " +
            "Please specify which Llama model to use. " +
            "Example: llmLlama({ baseURL: 'http://localhost:11434', model: 'llama3.2:3b' })");
    }
    const model = cfg.model;
    const options = cfg.options || {};
    let client = cfg.client;
    if (!client && (cfg.apiKey || cfg.baseURL)) {
        const base = (cfg.baseURL || "http://localhost:11434").replace(/\/$/, "");
        const apiKey = cfg.apiKey;
        client = {
            chat: {
                completions: {
                    create: async (params) => {
                        const res = await fetch(`${base}/v1/chat/completions`, {
                            method: "POST",
                            headers: {
                                "content-type": "application/json",
                                ...(apiKey ? { Authorization: `Bearer ${apiKey}` } : {}),
                            },
                            body: JSON.stringify(params),
                        });
                        if (!res.ok) {
                            const text = await res.text();
                            const err = new Error(`Llama HTTP ${res.status}`);
                            err.status = res.status;
                            err.body = text;
                            throw err;
                        }
                        // Handle streaming responses
                        if (params.stream) {
                            return res; // Return the response object for streaming
                        }
                        return await res.json();
                    },
                },
            },
        };
    }
    if (!client) {
        throw new Error("llmLlama: Missing configuration. " +
            "Please provide either 'client' or 'baseURL' for an OpenAI-compatible endpoint (e.g., Ollama). " +
            "Example: llmLlama({ baseURL: 'http://localhost:11434', model: 'llama3.2:3b' })");
    }
    return {
        id: `Llama-${model}`,
        model,
        client,
        async gen(prompt) {
            const resp = await client.chat.completions.create({
                model,
                messages: [{ role: "user", content: prompt }],
                ...options,
            });
            const msg = resp?.choices?.[0]?.message?.content ?? resp?.choices?.[0]?.text ?? "";
            return typeof msg === "string" ? msg : JSON.stringify(msg);
        },
        async genWithTools(prompt, tools) {
            const { nameMap, formattedTools } = createOpenAICompatibleTools(tools);
            const resp = await client.chat.completions.create({
                model,
                messages: [{ role: "user", content: prompt }],
                tools: formattedTools,
                tool_choice: "auto",
                ...options,
            });
            const message = resp?.choices?.[0]?.message;
            return parseOpenAICompatibleResponse(message, nameMap);
        },
        async *genStream(prompt) {
            const streamResponse = await client.chat.completions.create({
                model,
                messages: [{ role: "user", content: prompt }],
                stream: true,
                ...options,
            });
            // Handle Server-Sent Events streaming
            if (streamResponse && streamResponse.body) {
                const reader = streamResponse.body.getReader();
                const decoder = new TextDecoder();
                let buffer = '';
                try {
                    while (true) {
                        const { done, value } = await reader.read();
                        if (done)
                            break;
                        buffer += decoder.decode(value, { stream: true });
                        const lines = buffer.split('\n');
                        buffer = lines.pop() || ''; // Keep incomplete line in buffer
                        for (const line of lines) {
                            if (line.trim() === '' || line.startsWith(':'))
                                continue;
                            if (line === 'data: [DONE]')
                                return;
                            if (line.startsWith('data: ')) {
                                try {
                                    const jsonData = line.slice(6); // Remove 'data: ' prefix
                                    const parsed = JSON.parse(jsonData);
                                    const delta = parsed?.choices?.[0]?.delta?.content;
                                    if (typeof delta === 'string' && delta.length > 0) {
                                        yield delta;
                                    }
                                }
                                catch {
                                    // Skip malformed JSON lines
                                    continue;
                                }
                            }
                        }
                    }
                }
                finally {
                    reader.releaseLock();
                }
                return;
            }
            // If no response body, something went wrong
            throw new Error('No response body received from Llama streaming endpoint');
        },
    };
}
