import type { LLMHandle } from "./types";
export type OpenAIOptions = {
    temperature?: number;
    max_tokens?: number;
    max_completion_tokens?: number;
    top_p?: number;
    frequency_penalty?: number;
    presence_penalty?: number;
    stop?: string | string[];
    seed?: number;
    response_format?: {
        type: "json_object" | "text";
    };
    n?: number;
    logit_bias?: Record<string, number>;
    user?: string;
};
export type JSONSchema = {
    type: "object";
    properties: Record<string, any>;
    required?: string[];
    additionalProperties?: boolean;
    [key: string]: any;
};
export type OpenAIResponsesOptions = Omit<OpenAIOptions, 'response_format'> & {
    jsonSchema: {
        name: string;
        description?: string;
        schema: JSONSchema;
        strict?: boolean;
    };
};
export type OpenAIResponsesConfig = {
    apiKey: string;
    model: string;
    baseURL?: string;
    options?: OpenAIResponsesOptions;
};
export type OpenAIConfig = {
    apiKey: string;
    model: string;
    baseURL?: string;
    options?: OpenAIOptions;
};
export declare function llmOpenAI(cfg: OpenAIConfig): LLMHandle;
/**
 * OpenAI LLM with Structured Outputs (JSON Schema validation)
 * Uses Chat Completions API with response_format set to json_schema mode.
 * Guarantees the model's output matches your JSON schema.
 *
 * Supported models: gpt-4o-mini, gpt-4o, gpt-4o-2024-08-06 and later, o1, o3
 */
export declare function llmOpenAIResponses(cfg: OpenAIResponsesConfig): LLMHandle;
